{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook:\n",
    "* Add Azure credentials specific to your Azure account in the configuration.json file. The configuration file should be in the same directory as this Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import azure.mgmt.batchai.models as models\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.file import FileService\n",
    "\n",
    "import utilities as utils\n",
    "from utilities.job_factory import ParameterSweep, NumericParameter, DiscreteParameter\n",
    "\n",
    "cfg = utils.config.Configuration('configuration.json')\n",
    "client = utils.config.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create resource group & batch AI workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.config.create_resource_group(cfg)\n",
    "_ = client.workspaces.create(cfg.resource_group, cfg.workspace, cfg.location).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file share "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File share\n",
    "azure_file_share_name = 'sat-solver'\n",
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload problem set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = 'max3sat-problems'\n",
    "azure_file_share_prefix_path = 'max3sat-problems'\n",
    "\n",
    "# Create the directory path in the cluster file share if necessary.\n",
    "if azure_file_share_prefix_path:\n",
    "    file_service.create_directory(\n",
    "        azure_file_share_name, azure_file_share_prefix_path, fail_on_exist=False)\n",
    "    \n",
    "def upload_all_files(directory):\n",
    "    for filename in [f for f in listdir(directory) if isfile(join(directory, f))]:\n",
    "        print(filename, end=' ')\n",
    "        file_service.create_file_from_path(\n",
    "            azure_file_share_name, azure_file_share_prefix_path, filename, local_dir + '/' + filename)\n",
    "    print()\n",
    "\n",
    "upload_all_files(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = 'algorithm'\n",
    "azure_file_share_prefix_path = ''\n",
    "\n",
    "# Create the directory path in the cluster file share if necessary.\n",
    "if azure_file_share_prefix_path:\n",
    "    file_service.create_directory(\n",
    "        azure_file_share_name, azure_file_share_prefix_path, fail_on_exist=False)\n",
    "\n",
    "def upload_file(filename):\n",
    "    file_service.create_file_from_path(\n",
    "        azure_file_share_name, azure_file_share_prefix_path, filename, local_dir + '/' + filename)\n",
    "\n",
    "upload_file('algorithm.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_count = 1\n",
    "cluster_name = 'sat-solver'\n",
    "vm_type = 'STANDARD_D1'\n",
    "setup_output_prefix = '/node-setup-logs'\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    vm_size=vm_type,\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        setup_task=models.SetupTask(\n",
    "            command_line='apt -y install gcc make',\n",
    "            std_out_err_path_prefix=setup_output_prefix\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Cluster Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utils.cluster.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'sweep001'\n",
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define sweep paramteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_specs = [\n",
    "#     NumericParameter(\n",
    "#         parameter_name=\"N\",\n",
    "#         data_type=\"INTEGER\",\n",
    "#         start=1,\n",
    "#         end=2,\n",
    "#         step=2,\n",
    "#         scale=\"LINEAR\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "param_specs = [\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"DURATION\",\n",
    "        values=[5000, 20000, 40000]\n",
    "    )\n",
    "]\n",
    "\n",
    "parameters = ParameterSweep(param_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create job template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_file_share_mount_path = 'afs'\n",
    "relative_code_path = f'{azure_file_share_mount_path}/{azure_file_share_prefix_path}'\n",
    "relative_data_path = f'{azure_file_share_mount_path}/max3sat-problems'\n",
    "\n",
    "\n",
    "jcp = models.JobCreateParameters(\n",
    "    cluster=models.ResourceId(id=cluster.id),\n",
    "    \n",
    "    node_count=1,\n",
    "    \n",
    "    std_out_err_path_prefix = f'$AZ_BATCHAI_JOB_MOUNT_ROOT/{azure_file_share_mount_path}',\n",
    "    \n",
    "    output_directories = [\n",
    "        models.OutputDirectory(\n",
    "            id='ALL',\n",
    "            path_prefix=f'$AZ_BATCHAI_JOB_MOUNT_ROOT/{azure_file_share_mount_path}'\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    mount_volumes = models.MountVolumes(\n",
    "        azure_file_shares=[\n",
    "            models.AzureFileShareReference(\n",
    "                account_name=cfg.storage_account_name,\n",
    "                credentials=models.AzureStorageCredentialsInfo(\n",
    "                    account_key=cfg.storage_account_key),\n",
    "                azure_file_url=f'https://{cfg.storage_account_name}.file.core.windows.net/{azure_file_share_name}',\n",
    "                relative_mount_path=azure_file_share_mount_path)\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    custom_toolkit_settings=models.CustomToolkitSettings(\n",
    "        command_line=f'tar xzf $AZ_BATCHAI_JOB_MOUNT_ROOT/{relative_code_path}/algorithm.tar.gz -C $AZ_BATCH_TASK_WORKING_DIR ; ./run.sh $AZ_BATCHAI_JOB_MOUNT_ROOT/{relative_data_path} v90c700 {parameters[\"DURATION\"]}'\n",
    "    )\n",
    "    \n",
    "    #, container_settings = \n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create jobs with specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    job_count = job_count + 1\n",
    "except NameError:\n",
    "    job_count = 1\n",
    "\n",
    "job_prefix = f'job{job_count:03d}'\n",
    "\n",
    "# Generate Jobs\n",
    "jobs_to_submit, param_combinations = parameters.generate_jobs(jcp)\n",
    "\n",
    "# Print the parameter combinations generated\n",
    "for idx, comb in enumerate(param_combinations):\n",
    "    print(f\"Parameters {idx + 1}: {comb}\")\n",
    "\n",
    "# Submit Jobs\n",
    "experiment_utils = utils.experiment.ExperimentUtils(client, cfg.resource_group, cfg.workspace, experiment_name)\n",
    "jobs = experiment_utils.submit_jobs(jobs_to_submit, job_prefix).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_extractor = utils.job.MetricExtractor(\n",
    "                        output_dir_id='stdouterr',\n",
    "                        logfile='stdout.txt',\n",
    "                        regex='metric = ([0-9]*)')\n",
    "\n",
    "\n",
    "# Wait for all jobs to complete\n",
    "experiment_utils.wait_all_jobs()\n",
    "\n",
    "# Get the metrics from the jobs\n",
    "results = experiment_utils.get_metrics_for_jobs(jobs, metric_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
