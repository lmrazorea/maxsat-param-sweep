{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook:\n",
    "* Add Azure credentials specific to your Azure account in the configuration.json file. The configuration file should be in the same directory as this Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import azure.mgmt.batchai.models as models\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.file import FileService\n",
    "\n",
    "import utilities as utils\n",
    "from utilities.job_factory import ParameterSweep, NumericParameter, DiscreteParameter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = utils.config.Configuration('configuration.json')\n",
    "client = utils.config.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create resource group & batch AI workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.config.create_resource_group(cfg)\n",
    "_ = client.workspaces.create(cfg.resource_group, cfg.workspace, cfg.location).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file share "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File share\n",
    "azure_file_share_name = 'sat-solver'\n",
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload problem set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = 'max3sat-problems'\n",
    "azure_file_share_prefix_path = 'max3sat-problems'\n",
    "\n",
    "# Create the directory path in the cluster file share if necessary.\n",
    "if azure_file_share_prefix_path:\n",
    "    file_service.create_directory(\n",
    "        azure_file_share_name, azure_file_share_prefix_path, fail_on_exist=False)\n",
    "    \n",
    "def upload_all_files(directory):\n",
    "    for filename in [f for f in listdir(directory) if isfile(join(directory, f))]:\n",
    "        print(filename, end=' ')\n",
    "        file_service.create_file_from_path(\n",
    "            azure_file_share_name, azure_file_share_prefix_path, filename, local_dir + '/' + filename)\n",
    "    print()\n",
    "\n",
    "upload_all_files(local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = 'algorithm'\n",
    "azure_file_share_prefix_path = ''\n",
    "\n",
    "# Create the directory path in the cluster file share if necessary.\n",
    "if azure_file_share_prefix_path:\n",
    "    file_service.create_directory(\n",
    "        azure_file_share_name, azure_file_share_prefix_path, fail_on_exist=False)\n",
    "\n",
    "def upload_file(filename):\n",
    "    file_service.create_file_from_path(\n",
    "        azure_file_share_name, azure_file_share_prefix_path, filename, local_dir + '/' + filename)\n",
    "\n",
    "upload_file('algorithm.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_count = 10\n",
    "cluster_name = 'sat-solver'\n",
    "vm_type = 'STANDARD_D1'\n",
    "setup_output_prefix = '/node-setup-logs'\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    vm_size=vm_type,\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        setup_task=models.SetupTask(\n",
    "            command_line='apt -y install gcc make bc',\n",
    "            std_out_err_path_prefix=setup_output_prefix\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Cluster Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utils.cluster.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'sweep001'\n",
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define sweep paramteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_specs = [\n",
    "#     NumericParameter(\n",
    "#         parameter_name=\"N\",\n",
    "#         data_type=\"INTEGER\",\n",
    "#         start=1,\n",
    "#         end=2,\n",
    "#         step=2,\n",
    "#         scale=\"LINEAR\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "param_specs = [\n",
    "    DiscreteParameter(\n",
    "        parameter_name=\"DURATION\",\n",
    "        values=[20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000]\n",
    "    )\n",
    "]\n",
    "\n",
    "parameters = ParameterSweep(param_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create job template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_file_share_mount_path = 'afs'\n",
    "relative_code_path = f'{azure_file_share_mount_path}/{azure_file_share_prefix_path}'\n",
    "relative_data_path = f'{azure_file_share_mount_path}/max3sat-problems'\n",
    "#problem_set_selector = 'v90c700'\n",
    "problem_set_selector = 'v90c900'\n",
    "\n",
    "jcp = models.JobCreateParameters(\n",
    "    cluster=models.ResourceId(id=cluster.id),\n",
    "    \n",
    "    node_count=1,\n",
    "    \n",
    "    std_out_err_path_prefix = f'$AZ_BATCHAI_JOB_MOUNT_ROOT/{azure_file_share_mount_path}',\n",
    "    \n",
    "    output_directories = [\n",
    "        models.OutputDirectory(\n",
    "            id='ALL',\n",
    "            path_prefix=f'$AZ_BATCHAI_JOB_MOUNT_ROOT/{azure_file_share_mount_path}'\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    mount_volumes = models.MountVolumes(\n",
    "        azure_file_shares=[\n",
    "            models.AzureFileShareReference(\n",
    "                account_name=cfg.storage_account_name,\n",
    "                credentials=models.AzureStorageCredentialsInfo(\n",
    "                    account_key=cfg.storage_account_key),\n",
    "                azure_file_url=f'https://{cfg.storage_account_name}.file.core.windows.net/{azure_file_share_name}',\n",
    "                relative_mount_path=azure_file_share_mount_path)\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    custom_toolkit_settings=models.CustomToolkitSettings(\n",
    "        command_line=f'tar xzf $AZ_BATCHAI_JOB_MOUNT_ROOT/{relative_code_path}/algorithm.tar.gz -C $AZ_BATCH_TASK_WORKING_DIR ; ./run.sh $AZ_BATCHAI_JOB_MOUNT_ROOT/{relative_data_path} {problem_set_selector} {parameters[\"DURATION\"]} $AZ_BATCHAI_OUTPUT_ALL'\n",
    "    )\n",
    "    \n",
    "    #, container_settings = \n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create jobs with specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    job_count = job_count + 1\n",
    "except NameError:\n",
    "    job_count = 1\n",
    "\n",
    "job_prefix = f'job{job_count:03d}'\n",
    "\n",
    "# Generate Jobs\n",
    "jobs_to_submit, param_combinations = parameters.generate_jobs(jcp)\n",
    "\n",
    "# Print the parameter combinations generated\n",
    "for idx, comb in enumerate(param_combinations):\n",
    "    print(f\"Parameters {idx + 1}: {comb}\")\n",
    "\n",
    "# Submit Jobs\n",
    "experiment_utils = utils.experiment.ExperimentUtils(client, cfg.resource_group, cfg.workspace, experiment_name)\n",
    "jobs = experiment_utils.submit_jobs(jobs_to_submit, job_prefix).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for all jobs to complete\n",
    "experiment_utils.wait_all_jobs()\n",
    "\n",
    "def print_metrics(metrics, metric_name='metric-val'):\n",
    "    for metric in metrics:\n",
    "        print(f'name: {metric[\"job_name\"]}, {metric_name}: {metric[\"metric_value\"]}, params: {[f\"{ev.name}: {ev.value}\" for ev in metric[\"job\"].environment_variables]}')\n",
    "\n",
    "def get_metric_values(metrics):\n",
    "    return [m[\"metric_value\"] for m in metrics]\n",
    "\n",
    "def get_metric_parameter_values(metrics, param_name='PARAM_DURATION'):\n",
    "    values = []\n",
    "    for env_vars in [ m[\"job\"].environment_variables for m in metrics ]:\n",
    "        for ev in env_vars:\n",
    "            if ev.name == param_name:\n",
    "                values.append(ev.value)\n",
    "    return values\n",
    "              \n",
    "def parameter_key(metric_instance, param_name='PARAM_DURATION'):\n",
    "    return [int(ev.value) for ev in metric_instance[\"job\"].environment_variables if ev.name == param_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print success ratios\n",
    "success_ratios_metric = experiment_utils.get_metrics_for_jobs(jobs, \n",
    "            utils.job.MetricExtractor(\n",
    "                output_dir_id='stdouterr',\n",
    "                logfile='stdout.txt',\n",
    "                regex='instance-success-ratio: ([0-9]+\\.?[0-9]*)',\n",
    "                calculate_method='all'\n",
    "            )\n",
    "        )\n",
    "\n",
    "success_ratios_metric.sort(key=parameter_key)\n",
    "print_metrics(success_ratios_metric, 'success-ratios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores\n",
    "scores_metric = experiment_utils.get_metrics_for_jobs(jobs, \n",
    "            utils.job.MetricExtractor(\n",
    "                output_dir_id='stdouterr',\n",
    "                logfile='stdout.txt',\n",
    "                regex='instance-score: ([0-9]+\\.?[0-9]*)',\n",
    "                calculate_method='all'\n",
    "            )\n",
    "        )\n",
    "\n",
    "scores_metric.sort(key=parameter_key)\n",
    "print_metrics(scores_metric, 'scores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data to plot from metrics\n",
    "success_ratios_data = get_metric_values(success_ratios_metric)\n",
    "print(scores_data)\n",
    "\n",
    "success_ratios_durations = get_metric_parameter_values(success_ratios_metric)\n",
    "print(scores_durations)\n",
    "\n",
    "\n",
    "scores_data = get_metric_values(scores_metric)\n",
    "print(scores_data)\n",
    "\n",
    "scores_durations = get_metric_parameter_values(scores_metric)\n",
    "print(scores_durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_errors(stds):\n",
    "    return list(map(lambda x: x / np.sqrt(len(stds)), stds))\n",
    "\n",
    "def std_err(data):\n",
    "    stds = np.std(data, axis=1)\n",
    "    return list(map(lambda x: x / np.sqrt(len(stds)), stds))\n",
    "\n",
    "def mean(data):\n",
    "    return np.mean(data, axis=1)\n",
    "\n",
    "def plot_mean_with_stderr(xrange, data, title, xlabel='', ylabel=''):\n",
    "    means = mean(data)\n",
    "    errors = std_err(data)\n",
    "    \n",
    "    plt.figure(figsize=(13.0, 6.0))\n",
    "    plt.errorbar(xrange, means, yerr=errors, fmt='o')\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_with_stderr(success_ratios_durations, success_ratios_data, f\"Success ratio vs. Duration for problem set '{problem_set_selector}'\", 'Durations', 'Success ratio')\n",
    "\n",
    "plot_mean_with_stderr(scores_durations, scores_data, f\"Score vs. Duration for problem set '{problem_set_selector}'\", 'Durations', 'Scores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
